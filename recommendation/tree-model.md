# 决策树

## boost和bagging

* boost。串行的方式训练基分类器，各分类器之间有依赖。每次训练时，对前一层基分类器分错的样本给与更高的权重
* bagging。bagging是Bootstrap aggregating的意思，各分类器之间无强依赖，可以并行。

## 方差和偏差

* 偏差:
    * 【背下来】偏差是指由有所采样得到的大小为m的训练数据集，训练出的所有模型的输出的平均值和真实模型输出之间的偏差。
    * 通常是由对学习算法做了错误的假设导致的
    * 描述模型输出结果的期望与样本真实结果的差距。分类器表达能力有限导致的系统性错误，表现在训练误差不收敛
* 方差
    * 【背下来】是指有所有采样得到的大小为m的训练数据集，训练出的所有模型的输出的方差
    * 描述模型对于给定值的输出稳定性。分类器对样本分布过于敏感，到指在训练样本较少的时候，出现过拟合

* 基分类器的错误，是偏差和方差之和
* boosting方法通过逐步聚焦分类器分错的样本，减少集成分类器的偏差
* Bagging采用分而治之的策略，通过对样本多次采样，分别训练多个模型，减少方差


为什么决策树是常用的基分类器
* 可以方便地将样本权重结合到训练过程中，不需要使用过采样的方法来调整样本权重
* 决策树的表达能力和泛化能力，可以通过调节树的层数来做折中
* 数据样本扰动对决策树影响较大，因此不同子样本集生成的基分类器随机性就较大。这样的不稳定学习器更适合作为基分类器

## GBDT

GBDT也称MART。基本的树：CART
* 拟合函数：负梯度
损失函数：
回归：直接用连续的值计算负梯度
均方差
绝对损失
huber损失
分位数损失
分类：指数（此时退化成为Adaboost）、对数
正则化:
步长，v就是步长，空驶学习速度 ( )= −1( )+ ℎ ( )
自采样比例（subsample）无放回的抽样
正则化剪枝
GBDT主要的优点有：

1) 可以灵活处理各种类型的数据，包括连续值和离散值。
2) 在相对少的调参时间情况下，预测的准确率也可以比较高。这个是相对SVM来说的。
3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。

from百面机器学习：
预测阶段计算速度较快，树与树之间可以并行化计算
在分布稠密的数据机上，泛化能力和表达能力都比较好
具有较好的解释性和鲁棒性
能够自动发现特征质检的高阶关系
不需要做特殊预处理（比如归一化）
GBDT的主要缺点有：

由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。
在高维稀疏数据上，表现不如SVM或神经网络
在处理文本分类特征问题上，相对其他模型优势不如在处理数值特征时明显
训练过程需要串行，只能在决策树内部采用一些局部并行手段提高训练速度


## Xgboost

https://zhuanlan.zhihu.com/p/148050748